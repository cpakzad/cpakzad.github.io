{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_boUx_3FvO4O"
   },
   "outputs": [],
   "source": [
    "###################### Magic commands\n",
    "\n",
    "##%%timeit -n 1 -r 1\n",
    "#%%time\n",
    "import time\n",
    "#start = time.time()\n",
    "\n",
    "###################### at the end\n",
    "\n",
    "#end = time.time()\n",
    "#print(end - start)\n",
    "\n",
    "###################### Packages\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import scipy.special\n",
    "import math \n",
    "import numba\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pip install fbm\n",
    "# from fbm import FBM\n",
    "\n",
    "###################### Options\n",
    "\n",
    "npr.seed(0)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#<\n",
    "#>\n",
    "\n",
    "############### Tools transfering numpy to numba\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def np_apply_along_axis_2darray(func1d, axis, arr):\n",
    "  assert arr.ndim == 2\n",
    "  assert axis in [0, 1]\n",
    "  if axis == 0:\n",
    "    result = np.empty(arr.shape[1])\n",
    "    for i in numba.prange(len(result)):\n",
    "      result[i] = func1d(arr[:, i])\n",
    "  else:\n",
    "    result = np.empty(arr.shape[0])\n",
    "    for i in numba.prange(len(result)):\n",
    "      result[i] = func1d(arr[i, :])\n",
    "  return result\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def np_mean_2darray(array, axis):\n",
    "  return np_apply_along_axis(np.mean, axis, array)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def np_std_2darray(array, axis):\n",
    "  return np_apply_along_axis(np.std, axis, array)\n",
    "\n",
    "################################################## Non-empty\n",
    "\n",
    "def check_cond(gamma,c,tau,q):\n",
    "    if 0<tau:\n",
    "        if gamma<1 and 0<gamma and 2<q and 0<c and 2*(c+tau)*gamma<1 and q*(1-2*tau*gamma)>2 and q*c*gamma>1 and tau <1/(2*gamma):\n",
    "            print('Valid!')\n",
    "        else:\n",
    "            print('Not valid!')\n",
    "    else:\n",
    "        if gamma<1 and 0<gamma and 2<q and 0<c and 2*(c+tau)*gamma<1 and q*c*gamma>1:\n",
    "            print('Valid!')\n",
    "        else:\n",
    "            print('Not valid!')        \n",
    "    return\n",
    "\n",
    "def check_cond_no_q(gamma,c,tau):\n",
    "    if gamma<1 and 0<gamma and 0<c and 2*(c+tau)*gamma<1:\n",
    "        print('Valid!')\n",
    "    else:\n",
    "        print('Not valid!')   \n",
    "    return\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=True)\n",
    "def find_cond_all(h_gamma,h_c,h_tau,h_q,max_c,max_tau,max_q):\n",
    "    gamma_mesh = np.linspace(0.01,0.99,h_gamma)\n",
    "    c_mesh = np.linspace(0.1,max_c,h_c)\n",
    "    tau_mesh = np.linspace(-max_tau,max_tau,h_tau)\n",
    "    q_mesh = np.linspace(2.1,max_q,h_q)\n",
    "    for gamma_index in numba.prange(h_gamma):\n",
    "        for c_index in numba.prange(h_c): \n",
    "            for tau_index in numba.prange(h_tau):\n",
    "                for q_index in numba.prange(h_q):\n",
    "                    gamma=gamma_mesh[gamma_index]\n",
    "                    c=c_mesh[c_index]\n",
    "                    tau=tau_mesh[tau_index]\n",
    "                    q=q_mesh[q_index]\n",
    "                    if 0<tau:\n",
    "                        if 2<q and 0<c and 2*(c+tau)*gamma<1 and q*(1-2*tau*gamma)>2 and q*c*gamma>1:\n",
    "                            print([gamma, c, tau, q])\n",
    "                    else:\n",
    "                        if 2<q and 0<c and 2*(c+tau)*gamma<1 and q*c*gamma>1:\n",
    "                            print([gamma, c, tau, q])                        \n",
    "    return 'Done!'\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def find_cond_no_q(grid_gamma,max_tau):\n",
    "    gamma_mesh = np.linspace(0,1,grid_gamma+1)[1:grid_gamma]\n",
    "    c_mesh = np.arange(1,10)\n",
    "    tau_mesh = np.arange(-max_tau,max_tau+1)\n",
    "    for gamma_index in numba.prange(gamma_mesh.shape[0]):\n",
    "        for c_index in numba.prange(c_mesh.shape[0]): \n",
    "            for tau_index in numba.prange(tau_mesh.shape[0]):\n",
    "                gamma=gamma_mesh[gamma_index]\n",
    "                c=c_mesh[c_index]\n",
    "                tau=tau_mesh[tau_index]\n",
    "                if 2*(c+tau)*gamma<1:\n",
    "                    print([gamma, c, tau])\n",
    "    return 'Done!'\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def find_gamma_no_q(grid_gamma,c,tau):\n",
    "    gamma_mesh = np.linspace(0,1,grid_gamma+1)[1:grid_gamma]\n",
    "    for gamma_index in numba.prange(gamma_mesh.shape[0]):\n",
    "        gamma=gamma_mesh[gamma_index]\n",
    "        if 2*(c+tau)*gamma<1:\n",
    "            print([gamma, c, tau])\n",
    "    return 'Done!'\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def find_cond_c_no_q(c,grid_gamma,max_tau):\n",
    "    gamma_mesh = np.linspace(0,1,grid_gamma+1)[1:grid_gamma]\n",
    "    tau_mesh = np.arange(-max_tau,max_tau+1)\n",
    "    for gamma_index in numba.prange(gamma_mesh.shape[0]):\n",
    "        for tau_index in numba.prange(tau_mesh.shape[0]):\n",
    "            gamma=gamma_mesh[gamma_index]\n",
    "            tau=tau_mesh[tau_index]\n",
    "            if 2*(c+tau)*gamma<1:\n",
    "                print([gamma, c, tau])\n",
    "    return 'Done!'\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def find_cond_c_tau_no_q(c,tau,grid_gamma):\n",
    "    gamma_mesh = np.linspace(0,1,grid_gamma+1)[1:grid_gamma]\n",
    "    tau_mesh = np.arange(-max_tau,max_tau+1)\n",
    "    for gamma_index in numba.prange(gamma_mesh.shape[0]):\n",
    "        for tau_index in numba.prange(tau_mesh.shape[0]):\n",
    "            gamma=gamma_mesh[gamma_index]\n",
    "            tau=tau_mesh[tau_index]\n",
    "            if 2*(c+tau)*gamma<1:\n",
    "                print([gamma, c, tau])\n",
    "    return 'Done!'\n",
    "\n",
    "################################################## Generating Data\n",
    "\n",
    "@numba.njit(parallel=False, fastmath=False)\n",
    "def Lomax_quantile_function(x,theta,s):  \n",
    "    return s*((1-x)**(-1/theta)-1)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def Pareto_quantile_function(x,gamma,s): # support is  {x \\ge s} and theta=1/\\gamma\n",
    "    return s*(1-x)**(-gamma)\n",
    "\n",
    "#The Burr distribution has survival distribution $\\bar{F}(y)=(1+y^\\rho)^{-\\theta} \\in 2\\RV_{-\\theta \\rho,-\\rho}$ where $x\\ge 0$ and $\\theta,\\rho>0$.\n",
    "@numba.njit(parallel=True, fastmath=False) # rho,theta positive\n",
    "def Burr_quantile_function(x,theta,rho): \n",
    "    return ((1-x)**(-1/theta)-1)**(1/rho)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func3(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.exp(-grid**2+grid)**2)/d)\n",
    "    return np.exp(-grid**2+grid)/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func2(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.exp(-grid)**2)/d)\n",
    "    return np.exp(-grid)/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.sin(2*np.pi*grid)**2)/d)\n",
    "    return np.sin(2*np.pi*grid)/norm\n",
    "\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def coeurjolly_cholesky_fbm_1D(d,H,sigma):\n",
    "    H2 = 2 * H\n",
    "    matcov = np.zeros((d-1,d-1))\n",
    "    for i in numba.prange(d-1):\n",
    "        for j in numba.prange(i,d-1):\n",
    "            r = (sigma**2)*(1/2)*(abs(i+1)**H2 + abs(j+1)**H2 - abs(j - i)**H2)\n",
    "            r = r/(d**H2)\n",
    "            matcov[i, j] = r\n",
    "            matcov[j, i] = matcov[i, j]\n",
    "    L = np.linalg.cholesky(matcov)\n",
    "    Z = npr.normal(0,1,size=(d - 1))\n",
    "    fBm = np.dot(L , Z)\n",
    "    #out=np.concatenate(([0], fBm))\n",
    "    # out=np.hstack(([0], fBm))\n",
    "    out= np.asarray([0] + list(fBm))\n",
    "    return out\n",
    "\n",
    "# @Article{RePEc:jss:jstsof:v:005:i07,\n",
    "#  author={Coeurjolly, Jean-Francois},\n",
    "#  title={{Simulation and identification of the fractional Brownian motion: a bibliographical and comparative study}},\n",
    "#  journal={Journal of Statistical Software},\n",
    "#  year=2000,\n",
    "#  volume={5},\n",
    "#  number={i07},\n",
    "#  pages={},\n",
    "#  month={},\n",
    "#  keywords={},\n",
    "#  doi={http://hdl.handle.net/10.18637/jss.v005.i07}\n",
    "#}\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def coeurjolly_cholesky_fbm_array(Z,H,sigma): # Z=npr.normal(0,1,size=(N,n,d - 1))\n",
    "    N=Z.shape[0]\n",
    "    n=Z.shape[1]\n",
    "    d=Z.shape[2]+1\n",
    "    out = np.zeros((N,n,d))\n",
    "    for p in numba.prange(N):\n",
    "        for q in numba.prange(n):    \n",
    "            H2 = 2 * H\n",
    "            matcov = np.zeros((d-1,d-1))\n",
    "            for i in numba.prange(d-1):\n",
    "                for j in numba.prange(i,d-1):\n",
    "                    r = (sigma**2)*(1/2)*(abs(i+1)**H2 + abs(j+1)**H2 - abs(j - i)**H2)\n",
    "                    r = r/(d**H2)\n",
    "                    matcov[i, j] = r\n",
    "                    matcov[j, i] = matcov[i, j]\n",
    "            L = np.linalg.cholesky(matcov)\n",
    "            fBm = np.dot(L , Z[p,q,:])\n",
    "            #out=np.concatenate(([0], fBm))\n",
    "            # out=np.hstack(([0], fBm))\n",
    "            out[p,q,:]= np.asarray([0] + list(fBm))\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def sigma(u,c,snr): \n",
    "    return (u**c)/snr\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def noise_mean(d,mu):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    return mu*grid\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu): #Z = npr.normal(0,1,size=(N,n,d - 1))  \n",
    "                                     #Y = Pareto_iterated_sample(N,n,theta,s)\n",
    "    N=Y.shape[0]\n",
    "    n=Y.shape[1]\n",
    "    d=Z.shape[2]+1\n",
    "    out = np.zeros((N,n,d))\n",
    "    H2 = 2 * H\n",
    "    matcov = np.zeros((d-1,d-1))\n",
    "    for p in numba.prange(N):\n",
    "        for q in numba.prange(n):\n",
    "            matcov = np.zeros((d-1,d-1))\n",
    "            for i in numba.prange(d-1):\n",
    "                for j in numba.prange(i,d-1):\n",
    "                    r = (sigma(Y[p,q],c,snr)**2)*(1/2)*(abs(i+1)**H2 + abs(j+1)**H2 - abs(j - i)**H2)\n",
    "                    r = r/(d**H2)\n",
    "                    matcov[i, j] = r\n",
    "                    matcov[j, i] = matcov[i, j]\n",
    "            L = np.linalg.cholesky(matcov)\n",
    "            fBm = np.dot(L , Z[p,q,:])\n",
    "            out[p,q,:]= np.asarray([0]+list(fBm)) + noise_mean(d,mu)\n",
    "    return out\n",
    "\n",
    "################################################## Estimation\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def threshold(X,Y,Y_sort_index,tau,m): # 1\\le k \\le n\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    y_matrix_out = np.zeros((N,n))\n",
    "    aux=concomittant_corr(X,Y,Y_sort_index,tau,int(n/5))[:,4:]\n",
    "    YY=np.copy(Y)\n",
    "    Y_sort=sort_2d_array(YY)\n",
    "    for i in numba.prange(N):\n",
    "        index = np.argmax(aux[i,:])\n",
    "        y_matrix_out[i,:] = Y_sort[i,n-index-1]\n",
    "    return y_matrix_out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def threshold_index(X,Y,Y_sort_index,tau,m): # 1\\le k \\le n\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    out=np.zeros((N,))\n",
    "    aux=concomittant_corr(X,Y,Y_sort_index,tau,int(n/5))[:,4:]\n",
    "    for i in numba.prange(N):\n",
    "        out[i] = np.argmax(aux[i,:])\n",
    "    return out\n",
    "\n",
    "#@numba.njit(parallel=True, fastmath=False) # It seems that \"greater_equal\" and numba don't work well together\n",
    "def fepls(X,Y,y_matrix,tau): # X of size (N,n,d) and Y of size (N,n)\n",
    "                             # y_matrix of shape (N,n), for instance y_matrix = y*np.ones((N,n)) where y threshold\n",
    "                             # tau is the tail index of \\vfi\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    out=np.zeros((N,d))\n",
    "    for j in range(d):\n",
    "        aux = np.multiply(X[:,:,j],Y**tau) # size (N,n,d) - Product \\vfi(Y_i)*X_i\n",
    "        out2 = np.multiply(aux,np.greater_equal(Y,y_matrix)) # size (N,n) - Product \\vfi(Y_i)*X_i*1_{Y_i \\ge y}\n",
    "        out[:,j]= np.sum(out2,axis=1)/n # (N,d)\n",
    "    norms=np.sqrt(np.sum(out**2,axis=1)/d) # length (N,)\n",
    "    out2 =  out * (norms.reshape((norms.size, 1)))**(-1)\n",
    "    return out2 # size (N,d)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def fepls_numba(X,Y,y_matrix,tau): # X of size (N,n,d) and Y of size (N,n)\n",
    "                             # y_matrix of shape (N,n), for instance y_matrix = y*np.ones((N,n)) where y threshold\n",
    "                             # tau is the tail index of \\vfi\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    out=np.zeros((N,d))\n",
    "    for j in numba.prange(d):\n",
    "        aux = np.multiply(X[:,:,j],Y**tau) # size (N,n,d) - Product \\vfi(Y_i)*X_i\n",
    "        out2 = np.multiply(aux,np.greater_equal(Y,y_matrix)) # size (N,n) - Product \\vfi(Y_i)*X_i*1_{Y_i \\ge y}\n",
    "        out[:,j]= np.sum(out2,axis=1)/n # (N,d)\n",
    "    norms=np.sqrt(np.sum(out**2,axis=1)/d) # length (N,)\n",
    "    out2 =  out * (norms.reshape((norms.size, 1)))**(-1)\n",
    "    return out2 # size (N,d)\n",
    "\n",
    "\n",
    "\n",
    "# g(t)= t^c with c \\in \\{1/4,1/2,1,3/2\\} \n",
    "# X = g(Y)\\beta + \\eps\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def Hill(Y,int_seq): # Y of size (N,n), y number, int_seq is an intermediate sequence, ie such that int_seq << n\n",
    "    N=Y.shape[0]\n",
    "    n=Y.shape[1]\n",
    "    Y_ord=np.copy(Y)\n",
    "    Y_ord=np.sort(Y_ord) \n",
    "    Y_2=Y_ord[:,n-int_seq-1]\n",
    "    aux=Y_ord/Y_2[:, None]\n",
    "    out=np.log(aux[:,0:n-int_seq])\n",
    "    return (1/int_seq)*np.sum(out,axis=1) # size (N,d)\n",
    "\n",
    "#### Same as np.sort for 2D arrays but works with numba njit+parallel\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def sort_2d_array(x):\n",
    "    n,m=np.shape(x)\n",
    "    for row in numba.prange(n):\n",
    "        x[row]=np.sort(x[row])\n",
    "    return x\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def hatbeta_dot_beta(X,Y,tau,l):\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    y_array=np.zeros((N,n,np.arange(int(n/l)).size))\n",
    "    out=np.zeros((N,np.arange(int(n/l)).size))\n",
    "    YY=np.copy(Y)\n",
    "    for p in numba.prange(int(n/l)):\n",
    "        y_array[:,0,p]=sort_2d_array(YY)[:,n-l*p-1]\n",
    "        for k in numba.prange(N):\n",
    "            y_array[k,:,p]=y_array[k,0,p]\n",
    "        hat_beta=fepls_numba(X,Y,y_array[:,:,p],tau) \n",
    "        out[:,p]=(1/d)*np.sum(np.multiply(hat_beta,X[:,p,:]),axis=1) \n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def concomittant_corr(X,Y,Y_sort_index,tau,m): # 1\\le m \\le n # Y_sort_index = np.argsort(Y,axis=1)\n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    d = X.shape[2]\n",
    "    out = np.zeros((N,m))\n",
    "    YY=np.copy(Y)\n",
    "    Y_sort=sort_2d_array(YY)\n",
    "    for k in numba.prange(m):\n",
    "        y_array = np.zeros((N,n,k+1))\n",
    "        aux = np.zeros((N,k+1))\n",
    "        aux2 = np.zeros((N,k+1))\n",
    "        aux3 = Y_sort[:,n-k-1:] # shape (N,k+1)\n",
    "        aux3_sum = np.sum(aux3,axis=1)\n",
    "        for i in numba.prange(k):\n",
    "            y_array[:,0,i] = Y_sort[:,n-i-1]\n",
    "            for j_2 in numba.prange(N):\n",
    "                y_array[j_2,:,i] = y_array[j_2,0,i]\n",
    "            hat_beta = fepls_numba(X,Y,y_array[:,:,i],tau) \n",
    "            for j_1 in numba.prange(N):\n",
    "                i_c = Y_sort_index[j_1,i]\n",
    "                aux[j_1,i]=(1/d)*np.sum(np.multiply(hat_beta[j_1,:],X[j_1,i_c,:]))\n",
    "                aux2[j_1,i]= np.multiply(aux[j_1,i],Y_sort[j_1,n-i-1]) \n",
    "                out[j_1,k]= np.corrcoef(aux3[j_1,:],aux[j_1,:])[0,1]\n",
    "    return out\n",
    "    \n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def hatbeta_dot_X(X,hat_beta,y_matrix,tau): # hat_beta=fepls(X,Y,y_matrix,tau) of shape (N,d)\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    out=np.zeros((N,n))\n",
    "    for i in numba.prange(n):\n",
    "            out[:,i]=(1/d)*np.sum(np.multiply(hat_beta,X[:,i,:]),axis=1) \n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def beta_dot_X(X):\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    out=np.zeros((N,n))\n",
    "    for i in numba.prange(n):\n",
    "            out[:,i]=(1/d)*np.sum(np.multiply(X[:,i,:],beta_func(d)),axis=1) \n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def conditional_cov_Y_hat_beta_X(X,Y,y_matrix,tau): #hat_beta = fepls(X,Y,y_matrix,tau)\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    A=hatbeta_dot_X(X,hat_beta,y_matrix,tau)\n",
    "    cov_A=np.zeros((N,))\n",
    "    cov_B=np.zeros((N,))\n",
    "    for k in numba.prange(N):\n",
    "        cov_A[k] = (((A[k,:])[Y[k,:]>y])*((Y[k,:])[Y[k,:]>y])).mean()-((A[k,:])[Y[k,:]>y]).mean()*((Y[k,:])[Y[k,:]>y]).mean()\n",
    "        #cov_B[k] = (((B[k,:])[Y[k,:]>y])*((Y[k,:])[Y[k,:]>y])).mean()-((B[k,:])[Y[k,:]>y]).mean()*((Y[k,:])[Y[k,:]>y]).mean()\n",
    "    return cov_A\n",
    "\n",
    "#@numba.njit(parallel=True, fastmath=False) # does not work with numba (no idea why)\n",
    "def conditional_cov_Y_beta_X(X,Y,y):\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    B=beta_dot_X(X)\n",
    "    cov_B=np.zeros((N,))\n",
    "    for k in numba.prange(N):\n",
    "        cov_B[k] = (((B[k,:])[Y[k,:]>y])*((Y[k,:])[Y[k,:]>y])).mean()-((B[k,:])[Y[k,:]>y]).mean()*((Y[k,:])[Y[k,:]>y]).mean()\n",
    "    return cov_B\n",
    "\n",
    "################################################## Execute\n",
    "\n",
    "N=500\n",
    "n=500\n",
    "d=101\n",
    "s_Y=1 # scale parameter of Pareto/Lomax distribution\n",
    "c= 1/2 # c is \\kappa the tail index of g\n",
    "tau=-2 # tail index of \\vfi\n",
    "#nu = 1 #  tail index of \\psi such that 2\\gamma \\nu_j < 1 for all 1\\le j \\le J\n",
    "snr=10 # signal-to-noise ratio\n",
    "H=1/3 # Hurst parameter of fBm noise\n",
    "gamma=9/10# 1/3 or 1/2 or 9/10\n",
    "rho=-1/2\n",
    "s = 3/4 # tunes the threshold y_n (see below)\n",
    "l=2 # grid parameter\n",
    "#y=n**(s*gamma) # threshold y_n\n",
    "#y_matrix = y*np.ones((N,n)) # threshold matrix\n",
    "mu = 200 # noise mean\n",
    "\n",
    "tic=time.time()\n",
    "\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "\n",
    "#############################################################################\n",
    "print(check_cond_no_q(gamma,c,tau))\n",
    "print(\"Time cost\",time.time()-tic)\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Histogram \n",
    "\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "y_matrix = threshold(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix[:,0], bins=m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################# Estim plot\n",
    "\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "y_matrix = threshold(X,Y,Y_sort_index,tau,m)\n",
    "\n",
    "E = fepls(X,Y,y_matrix,tau)\n",
    "\n",
    "# Calculate the maximum and minimum values for each position\n",
    "#max_values = np.max(E, axis=0) # shape (d,)\n",
    "#min_values = np.min(E, axis=0) # shape (d,)\n",
    "max_values = np.nanquantile(E, 0.95, axis=0) # shape (d,)\n",
    "min_values = np.nanquantile(E, 0.05, axis=0) # shape (d,)\n",
    "mean_values = np.nanmean(E, axis=0) # shape (d,)\n",
    "median_values = np.nanmedian(E, axis=0) # shape (d,)\n",
    "\n",
    "# Create x values (assuming x values are just indices in this case)\n",
    "x_values = np.arange(d)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Set x-axis tick labels to discretized interval [0,1]\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "ax.set_xticklabels([0,0 , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ])\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "ax.fill_between(x_values, min_values, max_values, color='skyblue', alpha=0.4)\n",
    "ax.plot(x_values, mean_values)\n",
    "ax.plot(x_values, beta_func(d))\n",
    "\n",
    "# Add labels and legend\n",
    "#ax.xlabel('Index')\n",
    "#ax.ylabel('Values')\n",
    "#ax.title('FEPLS Estimation of beta - Confidence intervals')\n",
    "#plt.legend()\n",
    "# Show the plot\n",
    "plt.savefig('beta_estim_plot_conc100_2_0_9.pdf')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Comparison tail index estimation with X and with <X,hat_beta>\n",
    "tic=time.time()\n",
    "alpha=0.8\n",
    "J=9\n",
    "h=mu\n",
    "h_vect=h*np.ones((N,))\n",
    "\n",
    "plot1=plot_tail_index_func(X,Y,h_vect,c,gamma,rho,alpha,J)\n",
    "\n",
    "hat_beta=fepls(X,Y,y_matrix,tau)\n",
    "X_dimred = hatbeta_dot_X(X,hat_beta,y_matrix,tau)\n",
    "plot2=plot_tail_index_uni(X_dimred,Y,h_vect,c,gamma,rho,alpha,J)\n",
    "\n",
    "max_values1 = np.nanquantile(plot1, 0.95, axis=0) # shape (d,)\n",
    "min_values1 = np.nanquantile(plot1, 0.05, axis=0) # shape (d,)\n",
    "mean_values1 = np.nanmean(plot1, axis=0) # shape (d,)\n",
    "median_values1 = np.nanmedian(plot1, axis=0) # shape (d,)\n",
    "\n",
    "max_values2 = np.nanquantile(plot2, 0.95, axis=0) # shape (d,)\n",
    "min_values2 = np.nanquantile(plot2, 0.05, axis=0) # shape (d,)\n",
    "mean_values2 = np.nanmean(plot2, axis=0) # shape (d,)\n",
    "median_values2 = np.nanmedian(plot2, axis=0) # shape (d,)\n",
    "\n",
    "# Create x values (assuming x values are just indices in this case)\n",
    "x_values = np.arange(d)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Set x-axis tick labels to discretized interval [0,1]\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "ax.set_xticklabels([0,0 , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ])\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "ax.fill_between(x_values, min_values1, max_values1, color='skyblue', alpha=0.5)\n",
    "ax.plot(x_values, mean_values1,color='blue')\n",
    "ax.fill_between(x_values, min_values2, max_values2, color='yellow', alpha=0.5)\n",
    "ax.plot(x_values, mean_values2,color='orange')\n",
    "ax.hlines(gamma,xmin=0,xmax=100,linewidth=1, color='r')\n",
    "\n",
    "plt.show()\n",
    "print(\"Time cost\",time.time()-tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### hatbeta_dot_beta\n",
    "A=hatbeta_dot_beta(X,Y,-2,l)\n",
    "A2=hatbeta_dot_beta(X,Y,-3,l)\n",
    "A3=hatbeta_dot_beta(X,Y,-4,l)\n",
    "\n",
    "\n",
    "# Calculate the maximum and minimum values for each position\n",
    "mean_values_A = np.nanmean(A, axis=0) # shape (int(n/l),)\n",
    "#median_values_A = np.nanmedian(A, axis=0) # shape (int(n/l),)\n",
    "\n",
    "mean_values_A2 = np.nanmean(A2, axis=0) # shape (int(n/l),)\n",
    "#median_values_A2 = np.nanmedian(A2, axis=0) # shape (int(n/l),)\n",
    "\n",
    "mean_values_A3 = np.nanmean(A3, axis=0) # shape (int(n/l),)\n",
    "#median_values_A3 = np.nanmedian(A3, axis=0) # shape (int(n/l),)\n",
    "\n",
    "# Create x values (assuming x values are just indices in this case)\n",
    "x_values_A = np.arange(int(n/l))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "#ax.fill_between(x_values_A, min_values_A, max_values_A, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A, label='tau = -2')\n",
    "\n",
    "#ax.fill_between(x_values_A, min_values_A2, max_values_A2, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A2, label='tau = -3')\n",
    "\n",
    "#ax.fill_between(x_values_A, min_values_A3, max_values_A3, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A3, label='tau = -4')\n",
    "\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "#ax.xlabel('Index')\n",
    "#ax.ylabel('Values')\n",
    "#ax.title('FEPLS Estimation of beta - Confidence intervals')\n",
    "plt.legend()\n",
    "plt.savefig('beta_estim_exceedance_2_0_9.pdf')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### Correlation (concomittant) plot\n",
    "\n",
    "m=int(n/2) # 1\\le m \\le n\n",
    "\n",
    "A=concomittant_corr(X,Y,Y_sort_index,-1,m)[:,1:]\n",
    "A2=concomittant_corr(X,Y,Y_sort_index,-2,m)[:,1:]\n",
    "A3=concomittant_corr(X,Y,Y_sort_index,-3,m)[:,1:]\n",
    "\n",
    "\n",
    "# Calculate the maximum and minimum values for each position\n",
    "\n",
    "mean_values_A = np.nanmean(A, axis=0) # shape (int(n/l),)\n",
    "#median_values_A = np.nanmedian(A, axis=0) # shape (int(n/l),)\n",
    "\n",
    "mean_values_A2 = np.nanmean(A2, axis=0) # shape (int(n/l),)\n",
    "#median_values_A2 = np.nanmedian(A2, axis=0) # shape (int(n/l),)\n",
    "\n",
    "mean_values_A3 = np.nanmean(A3, axis=0) # shape (int(n/l),)\n",
    "#median_values_A3 = np.nanmedian(A3, axis=0) # shape (int(n/l),)\n",
    "\n",
    "# Create x values (assuming x values are just indices in this case)\n",
    "x_values_A = np.arange(m-1)#np.arange(int(n/l))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Set x-axis tick labels to discretized interval [1,100]\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "ax.set_xticklabels([1, 1, 10 , 20, 30,  40, 50, 60, 70, 80, 90, 100 ])\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "#ax.fill_between(x_values_A, min_values_A, max_values_A, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A, label='tau = -1')\n",
    "\n",
    "#ax.fill_between(x_values_A, min_values_A2, max_values_A2, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A2, label='tau = -2')\n",
    "\n",
    "#ax.fill_between(x_values_A, min_values_A3, max_values_A3, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A3, label='tau = -3')\n",
    "\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "#ax.xlabel('Index')\n",
    "#ax.ylabel('Values')\n",
    "#ax.title('FEPLS Estimation of beta - Confidence intervals')\n",
    "plt.legend()\n",
    "#plt.savefig('beta_estim_corr_2_0_9.pdf')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=500\n",
    "n=500\n",
    "d=101\n",
    "s_Y=1 # scale parameter of Pareto/Lomax distribution\n",
    "tau=-2 # tail index of \\vfi\n",
    "#nu = 1 #  tail index of \\psi such that 2\\gamma \\nu_j < 1 for all 1\\le j \\le J\n",
    "snr=10 # signal-to-noise ratio\n",
    "H=1/3 # Hurst parameter of fBm noise\n",
    "rho=-1/2\n",
    "s = 3/4 # tunes the threshold y_n (see below)\n",
    "l=2 # grid parameter\n",
    "#y=n**(s*gamma) # threshold y_n\n",
    "#y_matrix = y*np.ones((N,n)) # threshold matrix\n",
    "mu = 200 # noise mean\n",
    "#theta=1/np.sqrt(gamma) # first parameter of Burr distribution \n",
    "#rho=1/np.sqrt(gamma) # second parameter of Burr distribution\n",
    "#theta=1 # first parameter of Burr distribution \n",
    "#rho=1/gamma # second parameter of Burr distribution\n",
    "\n",
    "\n",
    "c= 2 # c is \\kappa the tail index of g\n",
    "gamma=9/10# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_2_0_9.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 2 # c is \\kappa the tail index of g\n",
    "gamma=1/2# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_2_0_5.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 2 # c is \\kappa the tail index of g\n",
    "gamma=1/3# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_2_0_333.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 1 # c is \\kappa the tail index of g\n",
    "gamma=9/10# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_1_0_9.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 1 # c is \\kappa the tail index of g\n",
    "gamma=1/2# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_1_0_5.pdf')  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "c= 1 # c is \\kappa the tail index of g\n",
    "gamma=1/3# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_1_0_333.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 1 # c is \\kappa the tail index of g\n",
    "gamma=9/10# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_1_0_9.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 3/2 # c is \\kappa the tail index of g\n",
    "gamma=1/2# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_1_5_0_5.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 3/2 # c is \\kappa the tail index of g\n",
    "gamma=1/3# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_conc_1_5_0_333.pdf')  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "c= 3/2 # c is \\kappa the tail index of g\n",
    "gamma=9/10# 1/3 or 1/2 or 9/10\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m)\n",
    "plt.hist(y_matrix,m)\n",
    "plt.savefig('hist_k_conc_1_5_0_9.pdf')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## Conditional quantile estimation\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def Epanechnikov_kernel(x): # x is a np.array of shape (p,q);     W=np.where(np.abs(x)<=1,1,0)\n",
    "    out = np.zeros_like(x)\n",
    "    x=np.asarray(x)\n",
    "    for i in numba.prange(x.shape[0]):\n",
    "        for j in numba.prange(x.shape[1]):\n",
    "            if x[i,j]<=1 and x[i,j]>=0:\n",
    "                out[i,j] = np.multiply(3/2,1-np.power(x[i,j],2))\n",
    "            else:\n",
    "                out[i,j]=0\n",
    "    return out\n",
    "    \n",
    "@numba.njit(parallel=False, fastmath=False) \n",
    "def Gaussian_kernel(x):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-0.5*np.power(x,2))\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def univariate_Nadaraya_weight(X_dimred,x,h_vect,c,gamma,rho): # X_dimred of shape (N,n) just as Y and X_dimred = hatbeta_dot_X(X,hat_beta,y_matrix,tau)\n",
    "    # hat_beta=fepls(X,Y,y_matrix,tau)\n",
    "    # h_vect.shape = (N,); c is kappa \n",
    "    # x in (0,1)\n",
    "    N=X_dimred.shape[0]\n",
    "    n=X_dimred.shape[1]\n",
    "    out=np.zeros((N,n))\n",
    "    for k in numba.prange(N):\n",
    "        K_h=Gaussian_kernel((X_dimred[k,:]-Burr_quantile_function(x,gamma,rho)**c)/h_vect[k]) # shape (N,)\n",
    "        out[k,:]=K_h/np.sum(K_h) \n",
    "    return out ### shape = (N,n)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def functional_Nadaraya_weight(X,x,h_vect,c,gamma,rho): # X.shape = (N,n,d); h_vect.shape = (N,); x in (0,1)\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    X_fake = (Burr_quantile_function(x,gamma,rho)**c)*beta_func(d) # shape is (d,)   \n",
    "    aux = np.zeros((N,n,d))\n",
    "    for p in numba.prange(d):\n",
    "        aux[:,:,p]=(X[:,:,p]-X_fake[p])**2\n",
    "    norm = np.sqrt((1/d)*np.sum(aux,axis=2))\n",
    "    out=np.zeros((N,n))\n",
    "    K_h=Epanechnikov_kernel(np.divide(norm,h_vect.reshape(-1,1)))\n",
    "    for k in numba.prange(N):\n",
    "        out[k,:]=K_h[k,:]/np.sum(K_h[k,:]) \n",
    "    return out ### shape = (N,n)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)   \n",
    "def weighted_quantile(data,weight,alpha):  # data.shape=weight.shape=(n,) \n",
    "    # alpha is the treshold in (0,1)\n",
    "    sorter = np.argsort(data)\n",
    "    data = data[sorter]\n",
    "    weight = weight[sorter]\n",
    "    weighted_quantiles = np.cumsum(weight) - 0.5 * weight\n",
    "    weighted_quantiles /= np.sum(weight)\n",
    "    return np.interp(alpha, weighted_quantiles, data)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def iterated_weq(Y,weight,alpha): # same treshold for all marginals and iterations # weq = weighted empirical quantile\n",
    "    # data.shape = (N,n); weight.shape=(N,n)\n",
    "    N=Y.shape[0]\n",
    "    n=Y.shape[1]\n",
    "    out = np.zeros((N,))\n",
    "    for k in numba.prange(N):\n",
    "        out[k]=weighted_quantile(Y[k,:],weight[k,:],alpha)\n",
    "    return out # shape = (N,)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def tail_index_gamma_estimator(Y,weight,alpha,J): \n",
    "    N=Y.shape[0]\n",
    "    subdivision=np.array([(1/s) for s in np.arange(1,J+1)] )\n",
    "    quantile_data2=iterated_weq(Y,weight,alpha) \n",
    "    out=np.zeros((N,))\n",
    "    aux=np.zeros((N,J))\n",
    "    for k in numba.prange(N):\n",
    "        for j in numba.prange(J):\n",
    "            quantile_data1=iterated_weq(Y,weight,1-subdivision[j]*(1-alpha))\n",
    "            aux[k,j] = np.log(quantile_data1[k])-np.log(quantile_data2[k])\n",
    "            aux[k,j] /= -np.sum(np.log(subdivision))\n",
    "        out[k] = np.sum(aux[k,:])\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)  \n",
    "def plot_tail_index_func(X,Y,h_vect,c,gamma,rho,alpha,J): #h_vect=h*np.ones((N,)); h=2*mu; J=9; alpha in (0,1)\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    x_grid = np.linspace(0,1,d)\n",
    "    out=np.zeros((N,d))\n",
    "    for k in numba.prange(d):\n",
    "        weight=functional_Nadaraya_weight(X,x_grid[k],h_vect,c,gamma,rho)\n",
    "        #weight=univariate_Nadaraya_weight(X,x_grid[k],h_vect,c,gamma,rho)            \n",
    "        out[:,k]=tail_index_gamma_estimator(Y,weight,alpha,J)\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)  \n",
    "def plot_tail_index_uni(X_dimred,Y,h_vect,c,gamma,rho,alpha,J): #h_vect=h*np.ones((N,)); h=2*mu; J=9; alpha in (0,1); c is kappa\n",
    "    N=X_dimred.shape[0]\n",
    "    n=X_dimred.shape[1]\n",
    "    x_grid = np.linspace(0,1,d)\n",
    "    out=np.zeros((N,d))\n",
    "    for k in numba.prange(d):\n",
    "        weight=univariate_Nadaraya_weight(X_dimred,x_grid[k],h_vect,c,gamma,rho)          \n",
    "        out[:,k]=tail_index_gamma_estimator(Y,weight,alpha,J)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Comparison tail index estimation with X and with <X,hat_beta>\n",
    "########################### New model for X and Y with latent variable Z uniform on [0,3^{-1/2}]\n",
    "##################### X=g(Y)\\beta+\\eps and Y = Burr_distribution with \\gamma(Z) = 2/3+Z^2  \n",
    "##### Still Draft\n",
    "\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def univariate_Nadaraya_weight_new(X_dimred_new,x,z,h_vect,c,rho): # X_dimred of shape (N,n) just as Y and X_dimred = hatbeta_dot_X(X,hat_beta,y_matrix,tau)\n",
    "    # hat_beta=fepls(X,Y,y_matrix,tau)\n",
    "    # h_vect.shape = (N,); c is kappa \n",
    "    # x in (0,1), z in [0,3^{-1/2}]\n",
    "    N=X_dimred_new.shape[0]\n",
    "    n=X_dimred_new.shape[1]\n",
    "    out=np.zeros((N,n))\n",
    "    for k in numba.prange(N):\n",
    "        K_h=Gaussian_kernel((X_dimred_new[k,:]-Burr_quantile_function(x,2/3+z**2,rho)**c)/h_vect[k]) # shape (N,)\n",
    "        out[k,:]=K_h/np.sum(K_h) \n",
    "    return out ### shape = (N,n)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def functional_Nadaraya_weight_new(X_new,x,z,h_vect,c,rho): # X.shape = (N,n,d); h_vect.shape = (N,); x in (0,1)\n",
    "    N=X_new.shape[0]\n",
    "    n=X_new.shape[1]\n",
    "    d=X_new.shape[2]\n",
    "    X_fake = (Burr_quantile_function(x,2/3+z**2,rho)**c)*beta_func(d) # shape is (d,)   \n",
    "    aux = np.zeros((N,n,d))\n",
    "    for p in numba.prange(d):\n",
    "        aux[:,:,p]=(X_new[:,:,p]-X_fake[p])**2\n",
    "    norm = np.sqrt((1/d)*np.sum(aux,axis=2))\n",
    "    out=np.zeros((N,n))\n",
    "    K_h=Epanechnikov_kernel(np.divide(norm,h_vect.reshape(-1,1)))\n",
    "    for k in numba.prange(N):\n",
    "        out[k,:]=K_h[k,:]/np.sum(K_h[k,:]) \n",
    "    return out ### shape = (N,n)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)  \n",
    "def plot_tail_index_func_new(X_new,Y_new,h_vect,c,rho,alpha,J): #h_vect=h*np.ones((N,)); h=2*mu; J=9; alpha in (0,1)\n",
    "    N=X_new.shape[0]\n",
    "    n=X_new.shape[1]\n",
    "    d=X_new.shape[2]\n",
    "    z_grid = np.linspace(0,3**(-1/2),d)\n",
    "    out=np.zeros((N,d))\n",
    "    for k in numba.prange(d):\n",
    "        weight=functional_Nadaraya_weight_new(X_new,x_grid[k],h_vect,z,c,rho)\n",
    "        out[:,k]=tail_index_gamma_estimator(Y_new,weight,alpha,J)\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)  \n",
    "def plot_tail_index_uni_new(X_dimred_new,Y_new,h_vect,c,rho,alpha,J): #h_vect=h*np.ones((N,)); h=2*mu; J=9; alpha in (0,1); c is kappa\n",
    "    N=X_dimred_new.shape[0]\n",
    "    n=X_dimred_new.shape[1]\n",
    "    z_grid = np.linspace(0,3**(-1/2),d)\n",
    "    out=np.zeros((N,d))\n",
    "    for k in numba.prange(d):\n",
    "        weight= univariate_Nadaraya_weight_new(X_dimred_new,x_grid[k],z,h_vect,c,rho)\n",
    "        out[:,k]=tail_index_gamma_estimator(Y_new,weight,alpha,J)\n",
    "    return out\n",
    "\n",
    "alpha=0.765\n",
    "J=9\n",
    "h=2*mu\n",
    "h_vect=h*np.ones((N,))\n",
    "\n",
    "def tail_index_of_Y(z):\n",
    "    return 2/3 + z**2\n",
    "\n",
    "Z_new = npr.uniform(0,3**(-1/2),size=(N,n)) # latent variables for both Y\n",
    "Y_new = Burr_quantile_function(npr.uniform(0,1,size=(N,n)),tail_index_of_Y(Z_new),rho)\n",
    "L_new=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux_new=np.multiply.outer(Y_new**c,beta_func(d)) # g(Y)\\beta\n",
    "eps_new=coeurjolly_cholesky_fbm_var(Y_new,L_new,H,c,snr,mu) \n",
    "X_new=aux_new+eps_new\n",
    "\n",
    "alpha=0.765\n",
    "J=9\n",
    "h=2*mu\n",
    "h_vect=h*np.ones((N,))\n",
    "\n",
    "plot1=plot_tail_index_func_new(X_new,Y_new,h_vect,c,rho,alpha,J)\n",
    "hat_beta_new=fepls(X_new,Y_new,y_matrix,tau)\n",
    "X_dimred_new = hatbeta_dot_X(X_new,hat_beta_new,y_matrix,tau)\n",
    "plot2=plot_tail_index_uni_new(X_dimred_new,Y_new,h_vect,c,rho,alpha,J)\n",
    "\n",
    "\n",
    "max_values1 = np.nanquantile(plot1, 0.95, axis=0) # shape (d,)\n",
    "min_values1 = np.nanquantile(plot1, 0.05, axis=0) # shape (d,)\n",
    "mean_values1 = np.nanmean(plot1, axis=0) # shape (d,)\n",
    "median_values1 = np.nanmedian(plot1, axis=0) # shape (d,)\n",
    "\n",
    "max_values2 = np.nanquantile(plot2, 0.95, axis=0) # shape (d,)\n",
    "min_values2 = np.nanquantile(plot2, 0.05, axis=0) # shape (d,)\n",
    "mean_values2 = np.nanmean(plot2, axis=0) # shape (d,)\n",
    "median_values2 = np.nanmedian(plot2, axis=0) # shape (d,)\n",
    "\n",
    "# Create x values (assuming x values are just indices in this case)\n",
    "x_values = np.arange(d)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Set x-axis tick labels to discretized interval [0,1]\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "ax.set_xticklabels([0,0 , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ])\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "ax.fill_between(x_values, min_values1, max_values1, color='skyblue', alpha=0.4)\n",
    "ax.plot(x_values, mean_values1)\n",
    "ax.fill_between(x_values, min_values2, max_values2, color='blue', alpha=0.2)\n",
    "ax.plot(x_values, mean_values2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
